---
layout: post
title:  入门－真实的 Computer Science & Technology
mathjax: true
comments: true
excerpt: "入门－真实的 Computer Science & Technology"
date:   2016-07-25 +1828
categories: lecture
---

这个是给CS方面新人深度科普用的

首先本人看上去就很弱，欢迎大家调教（咦？）

－－－正文－－－

很不幸的是，计算机是一个极其大，内容极其多的学科。如果之前你以为就是写写代码或者设计电路那就错了。	

计算机主要问题是计算，“计算”wikipedia定义是对信息的处理，所以计算机工业也往往叫IT（信息技术）


## 计算机的来源

### 能造出这样一种机器的理由

能够有一种模型，只要人们能够明确表述出某种计算或者操作的具体规则，它就模仿人做到相同的事情？这个一点都不显然。

图灵构造性证明了：可以。这种模型叫`图灵机`。实现图灵机同等功能的机器叫`计算机`。

这种操作或者具体规则叫`算法`。

### 从逻辑到硬件

图灵机是符号逻辑，算法是人们的思维逻辑，真实的硬件理解的是二进制编码的逻辑（电路啊电路啊不这么做电路哭死啊）

人们很快填充了算法和硬件两者之间的东西。由此创造出了计算机这个专业以及它的专业词汇。

- 算法所依赖的数据关系叫数据结构。描述算法的逻辑形式叫程序。设计程序的入门课程叫程序设计。

- 程序的内容叫代码，写代码的行为叫编程，（专业）写代码的人叫程序员，代码的具体规则叫（计算机）语言。

- 把代码（逻辑上的程序）变成计算机硬件可以识别的程序的过程叫`编译`。执行这个过程的程序叫编译器。（C，C++，Fortran，Pascal等语言大多依赖编译器）

- 计算机硬件可以识别的程序往往又叫做`二进制程序`（因为此时的代码是二进制形式），相应的编译前的（逻辑层次的）的代码叫特指为`源代码`。

- 能够直接读取源代码并执行其功能的程序叫`解释器`，它（just-in-time模式的例外）不需要编译。（python, lua, bash, perl, php, javascript等语言大多依赖解释器），由于没有对应的二进制程序，它们的源代码又往往称为`脚本`。这种语言也称为`脚本语言`

- 管理和协调程序的程序集合叫`操作系统`（也可以视为软件／硬件中间的逻辑层次，因为有了操作系统后其他程序基本不用直接关心硬件（用于特定硬件的驱动程序除外））

- 模拟真实计算机硬件的软件叫`虚拟机（软件）`

操作系统的出现大大增加了计算机词汇。这里不一一列举了。


### 计算机文化

- 精通计算机原理和软件缺陷的dalao叫`黑客`。利用缺陷进行破坏的叫`骇客`（两词有时候被混用）

- 第一个异常有名的用于编写程序的软件叫`Emacs`，它被人加入越来越多的功能，有人甚至用它控制煮咖啡(真有一个煮咖啡标准协议的)。后面出现一个同样有名的叫`Vi`，它的改进版叫`ViM`。它们各自的用户经常互相指责对方（最好的编辑器）［误

- 不给用户展示源代码的应用叫`闭源应用`，相关的行为叫`闭源`。反之叫`开源`。

- 比较早的应用很广的操作系统源于Unix（包括现在苹果的操作系统的来源）。后来微软自立门户搞了Windows。它们大多都是闭源的。之后Linus对Unix不满开发了Linux（这个名称是编辑取的，不是本人。另外狭义的Linux只包括Linux内核）。Linux系统的不同版本系列叫`发行版`，Android就是其中之一。Linux的出现很大程度改变了服务器和嵌入式设备的生态。

- 用于通用计算的单元叫`CPU`，图形计算的叫`GPU`。Nvidia和AMD的GPU被各自称为N卡和A卡（至于和核弹关系自己搜去）。

- C语言出来之前不少操作系统使用Pascal语言编写。

- Lisp原意是为了人工智能，现在恐怕用的最多的情形之一是给Emacs写插件

- Java是比较早的试图应用于网络应用的语言，当时Netscape觉得Java名气大就命名浏览器中的脚本语言叫Javascript（两者没有什么关系）并且这个语言10天就被设计好了。现在这两个语言都很火，也都列在被吐槽的前几位。

- Intel 奔腾系列的某个芯片由于打表打错了造成了运算错误。这可能是最严重的芯片设计错误之一。

- PHP被你们玩坏了

- Python是Guido在圣诞节觉得无聊写出来的

- 待补充

## 计算机的支撑体系

支撑体系让计算机成了能够实用的东西。

计算就要算的快，算得好。

由于历史的原因，计算机支撑体系大致分为软件和硬件。硬件的任务是提供基本的操作和计算能力，软件的任务是组织和利用这些计算能力解决实际问题。

### 算的快

如何算的快？

- 硬件要好。一个很烂的硬件很难发挥性能。硬件方面存在诸多矛盾，比如越专用的硬件越快，应用范围越窄，价格越高；越通用的硬件就相反，所以不能指望硬件一劳永逸，它永远是要发展的。此外硬件的灵活性一直是个问题（毕竟造出来就定型了，但是近年来FPGA等在改变这个情况）。于是衍生了芯片工艺、体系结构、硬件层次、用于设计硬件的编程（HDL）、并行（不同粒度，从指令到任务到整台机器）等诸多问题。
- 软件要适应硬件，充分发挥硬件性能，由此有了并行、并发、同步、异步这些控制软件执行的方式，这样才能充分利用硬件。由此衍生了同步控制（原子锁、互斥量等等）、并行编程（针对不同的硬件，CPU，GPU等等）、（并行）软件验证（如何保证和证明一个并行的程序按照自己的想法运行？出错了如何分析？）、包括异步／回调处理各种请求的各种方式（当今的很多能承受高负载的网络框架，用户体验好的前端）等等。
- 软件能够用巧妙的方法较快的解决问题。这个就是算法，而且一般是指普适性的算法（具体问题的算法差异和这个很大）。但是算法本身也不简单，比如如何设计一个算法？如何分析具体算法能有多快？如果算法有随机性，那么平均如何？某个问题最快的随机算法平均是否会快于确定的算法？是否所有的随机都可以用伪随机数模拟？针对某个问题，能设计出的最快的算法能有多快？或者是否存在比某个算法更快的算法？某些算法是否可以很容易的互相转换，从而可以视作一类？某个问题能够设计出的最好的算法按照计算难度可以分成哪些类，这些类之间的关系是什么？如果需要并行，算法又该如何设计？如果是量子计算机，又该如何设计更加高效的算法？这些问题的答案都不是很平庸和明显的。
- 计算任务需要合理管理，于是有了操作系统。这方面又有很多很多问题需要解决。比如如何分配和调节各个“任务”（进程）的资源？如何调度它们的工作顺序和时间（优先级／紧迫性／实时性／交互性）？如何保证系统的容错性和计算结果的一致性（比如突然断电了如何保证恢复时可以知道断电前哪些东西其实没有完成这样可以重置它们以免造成更多的错误）？如何保证系统的安全性？如何控制系统中的各种权限？如何管理用户，用户权限和密码？如何让操作系统跑在不同的硬件平台上（不是每个不同型号的笔记本都要重新设计一个操作系统）？如何识别管理外部硬件（想想有多少种型号的U盘，移动硬盘，显示器，投影仪，蓝牙，Wi-Fi，网线，光盘驱动器，键盘，鼠标，有线的和无线的，打印机，集成显卡，独立显卡，电源管理，音响耳机）？如何和外部计算机互联／共享，网络通信？系统太复杂之后又出现了代表核心逻辑的“内核”，内核本身的结构也是受到高度重视的。
- 。。。

以上各个问题都形成各个方向。

### 算的好

- 正确性。如何测试一个大型项目？如何证明代码不会有和你的想法不一致的行为（算法上面的逻辑漏洞，边界检查问题）？如何追踪有问题的程序？如何调试多线程应用？如何应对传输的时候因干扰产生的错误？如何解决死锁问题？硬件中分支预测／分支断定出错如何恢复？

- 一致性。如何保证并行／并发时数据的一致性（同时操作一个数会出错吗，有多个cache时一个改变内容如何同步其他的cache等等）？故障停止时如何鉴定哪些内容是完成的，哪些有可能没有全部完成需要清除？突发中断时如何保证恢复时还能正常执行？

- 安全性。如何验证对方身份？如何保证传输的数据没有被篡改？如何在开放信道下验证身份（比如你和Wi-Fi间的任何信号都可以被监测到，这时候如何保证一个陌生人连接Wi-Fi输入密码验证时不会被其他人利用）？如何避免对方重发你上一次发送的加密消息造成意外？如何确定操作者是人而不是机器？等等等等。这些是信息安全研究的内容。

- 实时性。如何保证一个系统总能在可以证明的时间内响应紧急事件？如何维持重要系统在很长时间内不会因为出问题或者升级等操作丧失功能？

- 功耗。如何设计硬件以降低功耗？程序行为是否也会影响功耗？如何调度任务实现节能？

等等。这些内容也针对不同方向。


## 计算机的开发体系

开发体系让计算机有足够的应用，并且一直处于前进和发展。

### 编程语言

编程语言作为基础可以以不同形式进行划分：

- 功能。适合写硬件驱动、系统内核的；适合高性能科学计算的；适合写用户前端，网页的；适合进行简单系统管理和批量操作的；适合被其他一些语言写成的程序调用并可以在线修改的；适合跨平台开发APP的；等等等等。常用的编程语言有几十种，有明确定义的可能有几百种。

- 范式。语言的语法特性（语句划分，运算符优先级，关键字等等）、类型特性，面向对象，函数式（函数式为主，或者采用map/reduce/filter等高阶函数以及匿名函数等），修饰器，类型推断，类型反射，动态类型，列表推导，序列化，标准容器，异步／多线程支持，迭代器，生成器，异常处理，作用域，编译引导，单／多继承，泛型／类型模版，值／引用传递方式，变量生存周期，垃圾回收等等。越现代和越新的标准往往使用越多的特性。

－ 执行模式。可以被编译为可执行程序，也可以生成中间码（Java/C#/Python的pyc），也可以作为可读脚本依赖解释器。

### 开发

这个是软件工程的内容。比如开发时如何多人协作，版本控制（git等），如何驱动开发流程（测试驱动开发，需求驱动），持续集成来确保尽早发现错误，软件／硬件文档等等等等。

### 测试验证

开发出的东西最终需要验证。对于非常苛刻的需求需要程序证明，一般的产品需要不同层次的测试，这些也可以在开发过程中部分完成。产品出来后需要有反馈机制，从用户得知问题，从社区获得开发者支援。

### 更新

产品设计有其生命周期，到达后或者因为需求或者决策需要进行更新。更新需要保证依赖链的正确性，并且需要兼容部分还未抛弃的旧标准。

### 平台接口，规范，协同

作为平台或者逻辑上的底层，需要向上层暴露必要接口以支持其发挥功能，亦或是某种功能需要统一规范（比如网络协议）。接口规范需要兼顾当前的使用需求、将来的可拓展性以及如果需要跨平台需要注意平台无关。使用接口者也需要了解如何使用才能发挥最大效益。

### 许可

是否开源？是否商业使用？如果是那么使用者需要遵守哪些规定？可以使用的范围如何？

总之，实际的工程开发是非常复杂的，非常考验人的系统能力。没有系统能力，纵使你知道火箭的每个零件的原理你也不能正确地生产并把它们正确地组装在一起并保证发射成功。

## 计算机目标

### 计算机能解决哪些问题

首先，计算机解决套路。一旦某个套路能够被明确描述为明确的一些操作，并且这些操作可以由被基础操作描述，那么计算机就解决这些问题相对人就是碾压优势。这个非常显然，把套路变成程序就是了，输入输出相应内容即可。

最早的套路是计算和模拟－－部分计算甚至电路底层已经支持了。现在应该没有人和当代计算机比计算能力了。

作为服务器后大量用于存储数据，注意这个是“存储”而不是“记忆”，因为人记东西和计算机完全不同，记忆的目的之一是存储，即保存一段数据一段时间后再原封不动地拿出来。

稍成熟之后计算机广泛用于业务处理（无论个人办公还是银行系统）－－很多业务都可以抽象为某种规则，然后写成程序就是了

之后人们将计算机用于多媒体（图形图像声音也是某种可以用套路编码的数据，游戏也是某些规则加上多媒体），并广泛用于娱乐等行业

网络出现后计算机连在一起，作为服务终端和客户端，这大大拓充了计算机的应用范围和应用场合。

最近几年嵌入式设备的成熟使得人们可以随时携带计算机（所谓“智能时代”）。

大数据时代，计算机用来处理海量数据，从中分析因果规律，甚至可以预见我们的数据拥有者可能比我们自己还了解自己。

直至现在，大多数行业和大多数人恐怕已经难以离开计算机了。计算机不仅在其本身领域上发展，也越来越多地渗透到其他领域（所谓交叉领域），去替代其中的套路和执行套路的岗位，也在创造越来越多的机会。

### 反思

如果仔细一点，你会发现实质上人们在越来越多地将控制权变得表层化－－就像曾经我们手动旋转按钮调频道，而现在我们手握一个遥控器，将实际操作交付给那个非常稳定的很少出错的计算机，正如现在支付宝你动动手指便能转移大量资金。

到这里，我们似乎有些觉得不对劲：我们做得对吗？

展望未来，又会如何？

首先，我们的“遥控器”会继续抽象，很快它连实体按键都没有，就是个显示按钮图像甚至模拟按压感觉的屏幕－－实际上现在已经做到了。到未来，可能连实体的屏幕都没有，只是你看到的影像－－实际上你操作的是空气，而这个也基本实现只不过没有普及而已。

也许，到未来，发射原子弹只是空气中的一个滑动－－甚至现在就有可能已经如此。

到这里，我想计算机也告诉了我们一个真理：什么叫控制。

现代人可能是这个星球上最脆弱的物种之一，然而却以绝对的优势控制着秩序，甚至影响这个星球的气候环境和未来命运。人能够将核反应控制到一个建筑内，不是因为人有更加强大的能量，而是人懂得控制－－这不是一个拼体力的过程，而是一整套精密的逻辑，对各种规律的极致应用。有了枪支之后人们可以独自面对野兽，不是因为人比古代变的强壮了，而是人们通过控制，利用火药的推动，将能量集中到致命的子弹上。而现在，我们越来越多地将这个逻辑交付给计算机－－核电站恐怕没有一个不靠计算机的维护，面对这种规模的控制人已经无能为力了。

这种控制权的转移，使得控制的原理变的越来越不重要－－你不用关心支付宝具体怎么实现转账的，整个过程，从传感器到图形显示到Java虚拟机到Android API到操作系统到系统内核到网络驱动到CPU内存到4G协议到信号发射到基站接收转发到网络路由到服务器端口到虚拟业务处理到数据库同步再将整个过程倒过来一直到接受方，其中还保证了安全性和稳定性（信号不好手机死机不会导致丢钱），恐怕清楚的人不多。这样使得人群日渐两极化－－设计者和使用者，知道原理的人占绝对的少数。在这个情况下，骇客和网络犯罪等多发也是正常现象，甚至不少企业也可能利用产品试图获得更多内容－－使用者又怎么知道呢？当多数人在享受结果的同时失去了真正的控制力后，不知将来的发展是喜是忧，可能相当长的时间内依赖于设计者的自觉性。

与此同时，人们还在试图挖掘计算机的终极潜力－－人工智能。近些年成效显著（即使离强AI差很远），甚至在打很多游戏上计算机渐渐超过最优秀的人类选手，并且输入是没有太多处理过的图像。我记得之前一段时间某科研团队开发了Quantum Move让人类选手玩，然后有些部分超过了最好的计算软件得到的结果。现在如果计算机在很多游戏上也超过人类，那么结果就很有意思－－计算机自己可以进行研究探索并且超过人类－－实际上已经在实现了，比如在药物分析和基因分析等层面。

回顾历史，当计算机进入一个领域时，一个重要的特征是－－原来的某些东西变便宜了，也就是期望价值变低了。某些行业甚至被消灭了，比如电话接线员等。智能计算取代思维劳动，机械化和机器人取代实体劳动，一直是在发生的事情；而计算机暂时不能取代的行业，相对保持它的价值。相信若干年后，我们再次环顾我们身边，我们会更加清楚的知道，什么才是人本身的价值？


