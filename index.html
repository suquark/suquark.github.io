<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>

  <title>Siyuan [Ryans] Zhuang (庄思源)</title>
  <meta name="description" content="Siyuan's homepage"/>

  <!-- Open Graph -->

  <!-- Bootstrap & MDB -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
    integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg=="
    crossorigin="anonymous"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css"
    integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q=="
    crossorigin="anonymous"/>

  <!-- Fonts & Icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
    integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
    crossorigin="anonymous"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css"
    integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg=="
    crossorigin="anonymous"/>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"/>

  <!-- Styles -->
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✨</text></svg>"/>
  <link rel="stylesheet" href="/assets/css/main.css"/>
  <link rel="canonical" href="/"/>

  <!-- Dark Mode -->
  <script type="text/javascript">
    // Has to be in the head tag, otherwise a flicker effect will occur.
    const setTheme = (theme) => {
      // transition
      document.documentElement.classList.add("transition");
      window.setTimeout(() => { document.documentElement.classList.remove("transition"); }, 500);
  
      if (theme) {
        document.documentElement.setAttribute("data-theme", theme);
      } else {
        document.documentElement.removeAttribute("data-theme");
      }
      localStorage.setItem("theme", theme);

      // Updates the background of medium-zoom overlay.
      if (typeof medium_zoom !== 'undefined') {
        medium_zoom.update({
          background: getComputedStyle(document.documentElement).getPropertyValue('--global-bg-color') + 'ee',  // + 'ee' for trasparency.
        });
      }
    };

    const getTheme = () => {
      let theme = localStorage.getItem("theme");
      if (theme == null) {
        const userPref = window.matchMedia;
        if (userPref && userPref('(prefers-color-scheme: dark)').matches) {
            theme = 'dark';
        }
      }
      return theme;
    };

    // Initialize theme
    setTheme(getTheme());

    document.addEventListener('DOMContentLoaded', function() {
        const mode_toggle = document.getElementById("light-toggle");
        mode_toggle.addEventListener("click", function() {
            // toggle theme
            setTheme(localStorage.getItem("theme") == "dark" ? "light" : "dark");
        });
    });
  </script>
</head>

<body class="fixed-top-nav">
  <!-- Header -->
  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
      <div class="container">

        <!-- Navbar Toggle -->
        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
          aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>

        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active"><a class="nav-link" href="/">about<span class="sr-only">(current)</span></a></li>

            <!-- Blog -->
            <li class="nav-item">
              <a class="nav-link" href="https://www.zhihu.com/people/siyz" target="_blank" rel="noopener noreferrer">zhihu (SIY.Z)</a>
            </li>
            <!-- Other pages -->
            <li class="nav-item"><a class="nav-link" href="https://scholar.google.com/citations?user=KSZmI5EAAAAJ" target="_blank" rel="noopener noreferrer">publications</a></li>

            <div class="toggle-container">
              <a id="light-toggle">
                <i class="fas fa-moon"></i>
                <i class="fas fa-sun"></i>
              </a>
            </div>

          </ul>
        </div>
      </div>
    </nav>
  </header>

  <!-- Content -->
  <div class="container mt-5">
    <div class="post">

      <header class="post-header">
        <h1 class="post-title">Siyuan [Ryans] Zhuang (庄思源)</h1>
        <p class="desc"><em>(He is also known as Ryan, SIY.Z, suquark.)</em></p>
      </header>

      <article>

        <div class="profile float-right">
          <img class="img-fluid z-depth-1 rounded"
               src="/assets/resized/1400x1763.jpg"
               srcset="/assets/resized/480x604.jpg 480w,/assets/resized/800x1007.jpg 800w,/assets/resized/1400x1763.jpg 1400w"/>
        </div>
        <div class="clearfix">
          <p>I am a final-year CS PhD candidate at UC Berkeley, advised by Professor
            <a href="https://people.eecs.berkeley.edu/~istoica/" target="_blank" rel="noopener noreferrer">Ion Stoica</a>
            and <a href="https://dawnsong.io/" target="_blank" rel="noopener noreferrer">Dawn Song</a>. I am affiliated to Berkeley’s <a href="https://sky.cs.berkeley.edu/" target="_blank"
              rel="noopener noreferrer">Sky Computing Lab</a>, and <a href="https://lmsys.org/" target="_blank" rel="noopener noreferrer">LMSYS Org</a>.
          </p>
          <p>Previously, I graduated from University of Science and Technology of China with a Bachelor’s degree in Computer Science (Honors Program).</p>
          <p>My current work focuses on <strong>large language models</strong>, <strong>machine learning system</strong>, and <strong>distributed system</strong>.</p>
        </div>

        <div class="news">
          <h2>recent news</h2>

          <div class="table-responsive">
            <table class="table table-sm table-borderless">
              <tr>
                <th scope="row">Dec 06, 2023</th>
                <td>Ph.D. advanced to candidacy</td>
              </tr>
              <tr>
                <th scope="row">Mar 30, 2023</th>
                <td>We release <a href="https://lmsys.org/blog/2023-03-30-vicuna/" target="_blank" rel="noopener noreferrer">Vicuna</a>: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality</td>
              </tr>
            </table>
          </div>

        </div>
        <div class="publications">
          <h2>selected publications</h2>
          <ol class="bibliography">
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div>
                <div id="lmsyschat" class="col-sm-8">
                  <div class="title">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset </div>
                  <div class="author">Lianmin Zheng*, Wei-Lin Chiang*, Ying Sheng, Tianle Li, <em>Siyuan Zhuang</em>, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang</div>
                  <div class="periodical"><em>In The Twelfth International Conference on Learning Representations, </em>2024</div>
                  <div class="links">
                    <a href="https://openreview.net/forum?id=BOfDKxfwt0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a>
                    <a href="https://arxiv.org/abs/2309.11998" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
                    <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                  </div>

                  <div class="abstract hidden">
                    <p>
                      Studying how people interact with large language models (LLMs) in real-world
                      scenarios is increasingly important due to their widespread use in various applications.
                      In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
                      containing one million real-world conversations with 25 state-of-the-art LLMs.
                      This dataset is collected from 210K unique IP addresses in the wild on our
                      Vicuna demo and Chatbot Arena website. We offer an overview of the dataset’s
                      content, including its curation process, basic statistics, and topic distribution,
                      highlighting its diversity, originality, and scale. We demonstrate its versatility
                      through four use cases: developing content moderation models that perform similarly to GPT-4,
                      building a safety benchmark, training instruction-following models that perform similarly to Vicuna,
                      and creating challenging benchmark questions.
                      We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities.
                      The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.
                    </p>
                  </div>

                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"> <abbr class="badge">SOSP</abbr> </div>
                <div id="vllm" class="col-sm-8">
                  <div class="title">Efficient Memory Management for Large Language Model Serving with PagedAttention</div>
                  <div class="author">Woosuk Kwon*, Zhuohan Li*, <em>Siyuan Zhuang</em>, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica</div>
                  <div class="periodical"><em>In Proceedings of the 29th Symposium on Operating Systems Principles (pp. 611-626), </em> 2023</div>
                  <div class="links">
                    <a href="https://dl.acm.org/doi/abs/10.1145/3600006.3613165" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
                    <a href="https://arxiv.org/abs/2309.06180" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
                    <a href="https://github.com/vllm-project/vllm" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">GitHub</a>
                    <a href="https://vllm.ai" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                  </div>
                  <div class="abstract hidden">
                    <p>High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time.
                      However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically.
                      When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size.
                      To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems.
                      On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage.
                      Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca.
                      The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms.
                    </p>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"> <abbr class="badge">NeurIPS</abbr> </div>
                <div id="jllmj" class="col-sm-8">
                  <div class="title">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</div>
                  <div class="author"> Lianmin Zheng*, Wei-Lin Chiang*, Ying Sheng*, <em>Siyuan Zhuang</em>, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica</div>
                  <div class="periodical"><em>Advances in Neural Information Processing Systems, 36, </em> 2023</div>
                  <div class="links">
                    <a href="https://openreview.net/forum?id=uccHPGDlao" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">OpenReview</a>
                    <a href="https://arxiv.org/abs/2306.05685" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                  </div>

                  <div class="abstract hidden">
                    <p>Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.
                      To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.
                      We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.
                      We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.
                      Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.
                      Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.
                      Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.
                      The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.
                    </p>
                  </div>

                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"> <abbr class="badge">OSDI</abbr> </div>
                <div id="exoflow" class="col-sm-8">
                  <div class="title">ExoFlow: A Universal Workflow System for Exactly-Once DAGs.</div>
                  <div class="author"><em>Siyuan Zhuang</em>, Stephanie Wang, Eric Liang, Yi Cheng, Ion Stoica</div>
                  <div class="periodical"><em>In 17th USENIX Symposium on Operating Systems Design and Implementation (pp. 269-286),</em> 2023</div>
                  <div class="links">
                    <a href="https://www.usenix.org/system/files/osdi23-zhuang.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                    <a href="https://github.com/suquark/ExoFlow" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">GitHub</a>
                  </div>

                  <div class="abstract hidden">
                    <p>Given the fundamental tradeoff between run-time and
                      recovery performance, current distributed systems often
                      build application-specific recovery strategies to minimize
                      overheads. However, it is increasingly common for different
                      applications to be composed into heterogeneous pipelines.
                      Implementing multiple interoperable recovery techniques
                      in the same system is rare and difficult. Thus, today’s users
                      must choose between: (1) building on a single system, and
                      face a fixed choice of performance vs. recovery overheads, or
                      (2) the challenging task of stitching together multiple systems
                      that can offer application-specific tradeoffs.
                      We present ExoFlow, a universal workflow system that enables a flexible choice of recovery vs. performance tradeoffs,
                      even within the same application. The key insight behind our
                      solution is to decouple execution from recovery and provide
                      exactly-once semantics as a separate layer from execution. For
                      generality, workflow tasks can return references that capture
                      arbitrary inter-task communication. To enable the workflow
                      system and therefore the end user to take control of recovery,
                      we design task annotations that specify execution semantics
                      such as nondeterminism. ExoFlow generalizes recovery for
                      existing workflow applications ranging from ETL pipelines
                      to stateful serverless workflows, while enabling further optimizations in task communication and recovery.
                    </p>
                  </div>

                </div>
              </div>
            </li>
            <li>
              <div class="row">
                <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div>
                <div id="terapipe" class="col-sm-8">
                  <div class="title">TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models</div>
                  <div class="author">Zhuohan Li, <em>Siyuan Zhuang</em>, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, Ion Stoica</div>
                  <div class="periodical"><em>In International Conference on Machine Learning (pp. 6543- 6552). PMLR.,</em> 2021</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2102.07988" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                    <a href="https://github.com/zhuohan123/terapipe" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">GitHub</a>
                  </div>
                  <div class="abstract hidden">
                    <p>Model parallelism has become a necessity for training modern large-scale deep language models.
                      In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property.
                      This enables a more fine-grained pipeline compared with previous work.
                      With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models.
                      We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration.
                      We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods.
                    </p>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="row">
                <div class="col-sm-2 abbr"> <abbr class="badge">SIGCOMM</abbr> </div>
                <div id="hoplite" class="col-sm-8">
                  <div class="title">Hoplite: Efficient Collective Communication for Task-Based Distributed Systems</div>
                  <div class="author"><em>Siyuan Zhuang</em>*, Zhuohan Li*, Danyang Zhuo, Stephanie Wang, Eric Liang, Robert Nishihara, Philipp Moritz, Ion Stoica</div>
                  <div class="periodical"><em>Proceedings of the 2021 ACM SIGCOMM 2021 Conference, Pages 641-656,</em> August 2021</div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2102.07988" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
                    <a href="https://dl.acm.org/doi/10.1145/3452296.3472897" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
                    <a class="abstract btn btn-sm z-depth-0" role="button">TL;DR</a>
                    <a href="https://github.com/suquark/hoplite" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">GitHub</a>
                  </div>
                  <div class="abstract hidden">
                    <p>Task-based distributed frameworks (e.g., Ray, Dask, Hydro) have become increasingly popular for distributed applications that contain asynchronous and dynamic workloads, including asynchronous gradient descent, reinforcement learning, and model serving.
                      As more data-intensive applications move to run on top of task-based systems, collective communication efficiency has become an important problem.
                      Unfortunately, traditional collective communication libraries (e.g., MPI, Horovod, NCCL) are an ill fit, because they require the communication schedule to be known before runtime and they do not provide fault tolerance.
                      We design and implement Hoplite, an efficient and fault-tolerant collective communication layer for task-based distributed systems.
                      Our key technique is to compute data transfer schedules on the fly and execute the schedules efficiently through fine-grained pipelining.
                      At the same time, when a task fails, the data transfer schedule adapts quickly to allow other tasks to keep making progress.
                      We apply Hoplite to a popular task-based distributed framework, Ray.
                      We show that Hoplite speeds up asynchronous stochastic gradient descent, reinforcement learning, and serving an ensemble of machine learning models that are difficult to execute efficiently with traditional collective communication by up to 7.8x, 3.9x, and 3.3x, respectively.
                    </p>
                  </div>
                </div>
              </div>
            </li>

          </ol>
        </div>
        <div class="social">
          <div class="contact-icons">
            <a href="mailto:%73.%7A@%62%65%72%6B%65%6C%65%79.%65%64%75"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=KSZmI5EAAAAJ&amp;hl=en" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/suquark/" title="GitHub" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/siyuanzhuang" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <!-- <a href="https://twitter.com/" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> -->
          </div>
          <div class="contact-note">s.z ^.^ berkeley.edu</div>
        </div>

      </article>

    </div>

  </div>

  <!-- Footer -->
  <footer class="fixed-bottom">
    <div class="container mt-0">
      © Copyright 2024 Siyuan Zhuang.
      A pure HTML website imitates the <a
        href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.
      Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div>
  </footer>
</body>

<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"
  integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg=="
  crossorigin="anonymous"></script>
<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js"
  integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A=="
  crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
  integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ=="
  crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js"
  integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw=="
  crossorigin="anonymous"></script>
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"
  integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>

<script type="text/javascript">
  $(document).ready(function () {
    // Init Masonry
    var $grid = $('.grid').masonry({gutter: 10, horizontalOrder: true, itemSelector: '.grid-item'});
    // Layout Masonry after each image loads
    $grid.imagesLoaded().progress(function() { $grid.masonry('layout'); });

    // Initialize medium zoom.
    medium_zoom = mediumZoom('[data-zoomable]', {
      // margin: 100,
      background: getComputedStyle(document.documentElement).getPropertyValue('--global-bg-color') + 'ee',  // + 'ee' for trasparency.
    });

    // Common JS
    $('a.abstract').click(function() {
        $(this).parent().parent().find(".abstract.hidden").toggleClass('open');
    });
    $('a.bibtex').click(function() {
        $(this).parent().parent().find(".bibtex.hidden").toggleClass('open');
    });
    $('.navbar-nav').find('a').removeClass('waves-effect waves-light');
  });

  // MathJax
  window.MathJax = {tex: {tags: 'ams'}};
</script>

<script defer type="text/javascript" id="MathJax-script"
  src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

</html>
